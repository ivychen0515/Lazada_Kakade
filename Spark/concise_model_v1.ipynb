{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/kakade/spark')\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.functions import udf,regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer,StopWordsRemover,CountVectorizer,VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "\n",
    "spark = SparkSession.builder.appName(\"kakade\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function define#####\n",
    "\n",
    "#count number of tokens in given column\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "#count total character length in title/description\n",
    "charCount = udf(lambda x: len(x),IntegerType())\n",
    "\n",
    "#count number of words w/o special/stop words in title/description\n",
    "def useful_token_func(col1,col2):\n",
    "    if len(col1)>col2:\n",
    "        return 0\n",
    "    else:\n",
    "        return col2-len(col1)\n",
    "usefulToken = udf(useful_token_func, IntegerType())\n",
    "\n",
    "#extract unique words from title\n",
    "uniqueExtract = udf(lambda words: list(set(words)),ArrayType(StringType()))\n",
    "\n",
    "#label string to int\n",
    "string2int = udf(lambda x: int(x),IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add header to data feature\n",
    "data_schema = [StructField(\"index\", IntegerType(), True), StructField(\"title\", StringType(), True),\\\n",
    "                  StructField(\"new_title\",StringType(), True), StructField(\"description\",StringType(), True), \\\n",
    "                  StructField(\"new_des\",StringType(), True), StructField(\"category_1\",StringType(), True), \\\n",
    "                  StructField(\"concise\",IntegerType(), True), StructField(\"clarity\", IntegerType(), True)]\n",
    "# country sku_id title category_1 category_2 category_3 short_description price product_type \n",
    "data_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data\n",
    "# train_data=spark.read.csv(\"/home/kakade/Support_files/With_catgory_index_Processed_title_des.csv\",escape='\"',header=True)\n",
    "train_data=spark.read.csv(\"/home/kakade/Support_files/With_catgory_index_Processed_title_des.csv\",escape='\"',schema=data_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only take samples from Fashion category\n",
    "test_sample=train_data.filter(train_data[\"category_1\"]==\"Fashion\").drop(\"category_1\")\n",
    "test_sample=test_sample.na.fill({\"description\":\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted features from part below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title feature extraction\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"new_title\", outputCol=\"original_words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"new_title\", outputCol=\"regex_words\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"regex_words\", outputCol=\"nsw_words\")\n",
    "\n",
    "\n",
    "#count title character length (including space and punctuation)\n",
    "test_1=test_sample.withColumn(\"title_character_count\",charCount(test_sample[\"title\"]))\n",
    "\n",
    "#count original title word \n",
    "test_1 = tokenizer.transform(test_1)\n",
    "test_1=test_1.withColumn(\"original_tokens\", countTokens(test_1[\"original_words\"]))\n",
    "\n",
    "#count special character(words) in title \n",
    "test_1 = regexTokenizer.transform(test_1)\n",
    "test_1=test_1.withColumn(\"regex_tokens\", usefulToken(test_1[\"regex_words\"],test_1[\"original_tokens\"]))\n",
    "\n",
    "#count stop words and informational words in title \n",
    "test_1 = stop_words_remover.transform(test_1)\n",
    "test_1=test_1.withColumn(\"sw_tokens\", usefulToken(test_1[\"nsw_words\"],test_1[\"original_tokens\"])-test_1[\"regex_tokens\"])\n",
    "test_1=test_1.withColumn(\"nsw_tokens\", countTokens(test_1[\"nsw_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate description and title relationship (num of informational words in title / num of informational words in des)\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"new_des\", outputCol=\"regex_des\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"regex_des\", outputCol=\"nsw_des\")\n",
    "\n",
    "test_2 = regexTokenizer.transform(test_1)\n",
    "test_2 = stop_words_remover.transform(test_2)\n",
    "test_2=test_2.withColumn(\"des_tokens\", countTokens(test_2[\"nsw_des\"]))\n",
    "test_2=test_2.withColumn(\"tit_des_ratio\",test_2[\"nsw_tokens\"]/test_2[\"des_tokens\"]).na.fill({\"tit_des_ratio\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def des_same_func(col1,col2):\n",
    "    return len(set(col1).intersection(set(col2)))\n",
    "des_same = udf(des_same_func, IntegerType())\n",
    "test_2 = test_2.withColumn(\"des_same\",des_same(\"regex_words\",\"nsw_des\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check unique words in title and count repeated times\n",
    "test_3=test_2.withColumn(\"unique_words\",uniqueExtract(test_2[\"nsw_words\"]))\n",
    "test_3=test_3.withColumn(\"repeat_tokens\",test_3[\"nsw_tokens\"]-countTokens(test_3[\"unique_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembling features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"original_tokens\", \"des_same\", \"nsw_tokens\",\"sw_tokens\", \"regex_tokens\",\"tit_des_ratio\",\"repeat_tokens\",\"title_character_count\",\"clarity\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "test_4 = assembler.transform(test_3)\n",
    "test_4=test_4.select(\"index\",\"title\",\"features\", test_4[\"concise\"].alias(\"label\"))\n",
    "\n",
    "# test_3.filter(\"index==1\").select(\"tokens\",\"sw_tokens\", \"regex_tokens\",\"r_tit_des\",\"count_repeat\").show()\n",
    "# test_4.select(\"index\",\"features\", test_4[\"concise\"].alias(\"label\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_test_split\n",
    "(trainingData, testData) = test_4.randomSplit([0.7, 0.3])\n",
    "\n",
    "#cross_validation_train setup\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "#LR model\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "paramGrid_lr = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .build()\n",
    "\n",
    "cv_lr = CrossValidator(estimator=lr,estimatorParamMaps=paramGrid_lr,evaluator=evaluator,numFolds=3)\n",
    "\n",
    "\n",
    "#Decision Tree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_dt = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxDepth, [2, 3, 5]).build()\n",
    "    \n",
    "cv_dt = CrossValidator(estimator=dt,estimatorParamMaps=paramGrid_dt,evaluator=evaluator,numFolds=3)    \n",
    "# Run Crossvalidation, and choose the best set of parameters.\n",
    "lr_model = cv_lr.fit(trainingData)\n",
    "dt_model = cv_dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy = 0.893959 \n",
      "DT accuracy = 0.869189 \n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "\n",
    "lr_prediction = lr_model.transform(testData)\n",
    "dt_prediction = dt_model.transform(testData)\n",
    "\n",
    "accuracy_lr = evaluator.evaluate(lr_prediction)\n",
    "accuracy_dt = evaluator.evaluate(dt_prediction)\n",
    "print(\"LR accuracy = %g \" % accuracy_lr)\n",
    "print(\"DT accuracy = %g \" % accuracy_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.106588\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "(trainingData, testData) = test_4.randomSplit([0.7, 0.3])\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "paramGrid_gbt = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxBins, [32,64,16]) \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5,7]) \\\n",
    "    .addGrid(gbt.minInfoGain, [0.0,2.0,5.0])\\\n",
    "    .build()\n",
    "cv_gbt = CrossValidator(estimator=gbt,estimatorParamMaps=paramGrid_gbt,evaluator=evaluator,numFolds=3)    \n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "gbt_model = cv_gbt.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "gbt_predictions = gbt_model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "# predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "accuracy_gbt = evaluator.evaluate(gbt_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy_gbt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_predictions.filter(\"label==1\").filter(\"prediction==0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeRegressionModel (uid=dtr_64f433770486) of depth 5 with 35 nodes, DecisionTreeRegressionModel (uid=dtr_1fa380606d2a) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_79c646e7e7cc) of depth 5 with 35 nodes, DecisionTreeRegressionModel (uid=dtr_dfd33197759c) of depth 5 with 31 nodes, DecisionTreeRegressionModel (uid=dtr_2f7afc8d4191) of depth 5 with 33 nodes, DecisionTreeRegressionModel (uid=dtr_cbf557db930e) of depth 5 with 39 nodes, DecisionTreeRegressionModel (uid=dtr_ef490b6d1e53) of depth 5 with 33 nodes, DecisionTreeRegressionModel (uid=dtr_ef530898e67a) of depth 5 with 37 nodes, DecisionTreeRegressionModel (uid=dtr_380ac6282c4d) of depth 5 with 35 nodes, DecisionTreeRegressionModel (uid=dtr_ff132cb71139) of depth 5 with 35 nodes]\n"
     ]
    }
   ],
   "source": [
    "999 811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
