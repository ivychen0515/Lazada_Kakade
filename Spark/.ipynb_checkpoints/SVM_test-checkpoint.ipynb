{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "import ast\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init('/home/kakade/spark')\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession, SQLContext\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# spark = SparkSession.builder.appName(\"kakade\").getOrCreate()\n",
    "path = '../Clean_Data_Frame.csv'\n",
    "df = spark.read.csv(path, escape='\"', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df.drop('concise', 'clarity', 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clarity=df.select('concise') \n",
    "train_concise = df.select('clarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "sentence = \"a b ,\" * 100 + \"a c ,\" * 10\n",
    "# localDoc = [sentence, sentence]\n",
    "doc = spark.sparkContext.parallelize(sentence).map(lambda line: line.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': [0.43905443, -0.7667189, -0.17813502, -0.3253162, 0.41634226, 0.76551837, -0.09081105, -0.653364, 0.5875519, 0.25033352], 'a': [-0.02687338, -0.022087062, -0.048573267, -0.021015614, -0.00106377, -0.03069951, -0.023969065, -0.0257689, 0.027737558, 0.010636443], ',': [-0.024809886, -0.045885265, -0.024989748, -0.043366857, 0.023501808, 0.013957346, 0.0045123817, 0.031325154, 0.010026073, 0.03852622], 'b': [0.020278096, -0.049461186, 0.016035479, -0.036945064, 0.02460109, 0.025221312, 0.02463982, -0.046255104, -0.02194646, 0.0067743124], 'c': [0.02434197, -0.022619968, -0.01764239, 0.006178373, -0.023531836, -0.020960461, 0.045801867, 0.011510628, 0.009533042, 0.0035524727]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec().setVectorSize(10).setSeed(42).fit(doc)\n",
    "model.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mm=model.getVectors()\n",
    "# sum=mm['a'][0]+mm['b'][0]+mm['c'][0]\n",
    "# sum\n",
    "a=[]\n",
    "try:\n",
    "    a=model.transform('m')\n",
    "except:\n",
    "    pass\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new=Word2VecModel.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataframe(filePath):\n",
    "\tdataframe = pd.read_csv(filePath)\n",
    "\tconcise = dataframe['concise']\n",
    "\tclarity = dataframe['clarity']\n",
    "\tx = dataframe.drop(['concise', 'clarity', 'index'], axis=1)\n",
    "\treturn x, clarity, concise\n",
    "\n",
    "def title_vec(title, w2v_model):\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttranslator = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\ttitle = title.translate(translator)\n",
    "\tword_tokens = word_tokenize(title)\n",
    "\twords = [w.lower() for w in word_tokens if not w in stop_words]\n",
    "\n",
    "\tvector = np.zeros(300)\n",
    "\tfor word in words:\n",
    "\t\ttry:\n",
    "\t\t\tvector += w2v_model[word]\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\treturn vector\n",
    "\n",
    "def evaluate_accuracy(predict, test):\n",
    "\tcount = 0\n",
    "\tfor i in range(len(predict)):\n",
    "\t\tif predict[i] == test[i]:\n",
    "\t\t\tcount += 1\n",
    "\treturn float(count) / len(predict)\n",
    "\n",
    "def read_vectors(filename):\n",
    "\tin_file = open(filename, 'r')\n",
    "\ttitle_vectors = []\n",
    "\tfor line in in_file:\n",
    "\t\ttitle_vectors.append(list(map(float, line.rstrip().split(' '))))\n",
    "\tin_file.close()\n",
    "\treturn title_vectors\n",
    "\n",
    "def write_vectors(filename, title_vectors):\n",
    "\tout_file = open(filename, 'w', encoding='utf-8')\n",
    "\tfor vec in title_vectors:\n",
    "\t\tfor x in vec:\n",
    "\t\t\tout_file.write(str(x))\n",
    "\t\t\tout_file.write(' ')\n",
    "\t\tout_file.write('\\n')\n",
    "\tout_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#desc_vectors = [title_vec(str(title), w2v_model) for title in train_x['description']]\n",
    "#title_vectors = [title_vec(title, w2v_model) for title in x['title']]\n",
    "#write_vectors('description_vector', desc_vectors)\n",
    "\n",
    "train_title_vectors = np.array(read_vectors('train_title_vectors'))\n",
    "train_clarity = np.array(train_clarity)\n",
    "cv = cross_validation.KFold(len(train_title_vectors), n_folds = 5)\n",
    "accuracy = 0\n",
    "for traincv, testcv in cv:\n",
    "    train_x = train_title_vectors[traincv]\n",
    "    train_y = train_clarity[traincv]\n",
    "    test_x = train_title_vectors[testcv]\n",
    "    test_y = train_clarity[testcv]\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_x, train_y)\n",
    "    accuracy += evaluate_accuracy(clf.predict(test_x), test_y) / 10\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
