{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/kakade/spark')\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "spark = SparkSession.builder.appName(\"kakade\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,FloatType,StructType\n",
    "#Add header to data feature\n",
    "feature_schema = [StructField(\"country\", StringType(), True), StructField(\"sku_id\", StringType(), True), \\\n",
    "                  StructField(\"title\",StringType(), True), StructField(\"category_1\",StringType(), True), \\\n",
    "                  StructField(\"category_2\",StringType(), True), StructField(\"category_3\",StringType(), True), \\\n",
    "                  StructField(\"description\",StringType(), True), StructField(\"org_price\", FloatType(), True), \\\n",
    "                  StructField(\"product_type\",StringType(), True)]\n",
    "# country sku_id title category_1 category_2 category_3 short_description price product_type \n",
    "feature_struc = StructType(fields=feature_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = spark.read.csv(\"../Data/training/data_train.csv\",escape='\"',schema=feature_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0c2b4caa3ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_feature.groupBy(\"category_1\").count().orderBy(\"count\",ascending=False).show(truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmain_category\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count>5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_category\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_feature.category_1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"main_category.category_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "#most other category only contain <=5 products\n",
    "#filter products out of the main categories\n",
    "# train_feature.groupBy(\"category_1\").count().orderBy(\"count\",ascending=False).show(truncate=False)\n",
    "main_category=train_feature.groupBy(\"category_1\").count().filter(\"count>5\")\n",
    "train_feature = train_feature.join(main_category, col(\"train_feature.category_1\") == col(\"main_category.category_1\"), 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer,StopWordsRemover,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_extract_func(col):\n",
    "    soup=BeautifulSoup(col)\n",
    "    return soup.get_text()\n",
    "    return (1-col1/col2)**2\n",
    "html_extract = udf(html_extract_func, StringType())\n",
    "train_feature = train_feature.withColumn(\"html_extract\",html_extract(\"description\"))\n",
    "# train_feature.select(\"html_extract\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- sku_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- category_1: string (nullable = true)\n",
      " |-- category_2: string (nullable = true)\n",
      " |-- category_3: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- org_price: float (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- html_extract: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|stop_words_filtered                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[material, non, sheer, shimmer, chiffonsizes, x, inches, x, inchescut, curved, ends]                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|[formulated, oil, free, hydrating, botanicals, remarkably, improves, skin, texture, abused, hands, restores, soft, smooth, refined, hands]                                                                                                                                                                                                                                                                                                                                           |\n",
      "|[cm, mini, microphone, compatible, iphone, various, smartphones, also, ipad, apple, computer, macbook, dual, headed, design, allows, two, people, using, simultaneously, features, high, sensitivity, omni, directional, sounds, output, perfect, audio, video, recording, mm, standard, connector, jack, convenient, clip, design, clip, collar, mm, standard, connector, jack, convenient, clip, design, clip, collar]                                                             |\n",
      "|[anmyna, complaint, silky, set, shampoo, ml, conditioner, ml, deep, nourish, repair, damaged, hair, protect, scalp, prevent, hair, loss]                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[authentic, rrefresh, brighten, skin, anti, wrinkle, deep, cleansing, effects]                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|[genuine, windows, hd, touch, intel, core, u, gb, ram, gb, rom, nvidia, geforce, gt, m, card, reader, sd, sdhc, vga, web, camera, asus, sonicmaster, lite, technology]                                                                                                                                                                                                                                                                                                               |\n",
      "|[color, temperature, k, ra, voltage, v, v, power, w, w, diameter, cm, inner, outer, weight, g]                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|[reviving, like, new, born, baby, age, apply, age, net, content, app, g, pair, guarantee, period, years]                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[designed, constructed, super, speed, usb, specifications, extend, usb, port, pc, laptop, practical, spot, desktop, enables, throughput, gbps, used, usb, host, device, backwards, compatible, usb, usb, specification, convenient, slim, flat, cable, design, ultra, flexible, tangle, free]                                                                                                                                                                                        |\n",
      "|[genuine, issued, mcdonald, coca, cola, merchandize, limited, edition, collection, brand, new, paper, packaging, color, purple, limited, stock, left, second, chance, ideal, putting, drink, beverage, even, coke, even, ideal, keeping, coca, cola, museum, free, shipping]                                                                                                                                                                                                         |\n",
      "|[stainless, steel, filter, stainless, tea, filter, tea, ball, filter]                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[material, genuine, prehnite, crystal, beads, size, mm, beads, length, cm, stretchy]                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|[lead, nickle, free, good, quality, free, shipping, enough, stock]                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[accurately, track, day, stats, like, steps, taken, distance, traveled, calories, burned, stairs, climbed, active, minutes, see, daily, stats, time, day, bright, oled, display, monitor, long, well, sleep, wake, silent, vibrating, alarm, get, call, notifications, right, wrist, access, real, time, run, stats, like, time, distance, pace, stay, track, sync, stats, wirelessly, automatically, computer, leading, smartphones]                                                |\n",
      "|[apply, morning, evening, eyelash, treatment, excellent, occular, tolerability]                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|[brand, new, high, quality, fashion, men, business, shoes, pu, leather, lace, shoes, type, formal, shoes, gender, men, material, pu, rubber, sole, color, brown, black, asian, size]                                                                                                                                                                                                                                                                                                 |\n",
      "|[gender, men, movement, mechanical, movement, type, automatic, self, wind, type, skeleton, watch, display, analog, case, material, alloy, dial, color, black, case, diameter, mm, case, diameter, approx, mm, case, thickness, approx, mm, band, color, black, band, material, stailess, steel, band, length, approx, cm, band, width, approx, cm, net, weight, kg]                                                                                                                  |\n",
      "|[print, sides, glossy, finish, diy, customize, images, designs, text, slim, profile, lightweight, impact, resistant, durable, hard, plastic, access, ports, controls, sensors, lay, flat, bezel, protect, screen, directly, contacting, surfaces, keep, nice, mood, every, day, best, valentine, day, christmas, birthday, hallowmas, little, gift, girls, boys, women, men, friends, family, new, fashionable, cute, lovely, funny, cool, useful, cheap, excellent, choices, jianse]|\n",
      "|[item, type, phone, holder, size, x, x, cm, material, alloy, pvc, compatible, phone, model, inches, color, light, blue]                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|[genuine, authentic, products, free, fast, shipping, buy, confidence]                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "des_cat_test = train_feature.select(\"category_1\",\"sku_id\",F.regexp_replace(train_feature.html_extract, '(\\d+)', ' ').alias('html_extract'))\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"html_extract\", outputCol=\"description_token\", pattern=\"\\\\W\")\n",
    "des_cat_test = regexTokenizer.transform(des_cat_test)\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"description_token\", outputCol=\"stop_words_filtered\")\n",
    "des_cat_test = stop_words_remover.transform(des_cat_test)\n",
    "des_cat_test.select(\"stop_words_filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##currency_exchange\n",
    "#set all price to PHP\n",
    "import pyspark.sql.functions as F\n",
    "from forex_python.converter import CurrencyRates\n",
    "cex = CurrencyRates()\n",
    "S2P=cex.get_rate(\"SGD\",\"PHP\")\n",
    "M2P=cex.get_rate(\"MYR\",\"PHP\")\n",
    "train_feature=train_feature.withColumn(\"price\", F.when(train_feature.country == \"my\", M2P*train_feature.org_price).when(train_feature.country == \"sg\", S2P*train_feature.org_price).otherwise(train_feature.org_price))\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|             price|org_price|\n",
      "+------------------+---------+\n",
      "|            597.31|     49.0|\n",
      "|           1560.32|    128.0|\n",
      "| 305.6032962799072|    25.07|\n",
      "|1438.4199999999998|    118.0|\n",
      "|1399.4120372009277|    114.8|\n",
      "|31681.809999999998|   2599.0|\n",
      "| 4741.787980957031|   388.99|\n",
      "|126.77599534988403|     10.4|\n",
      "|               0.0|      0.0|\n",
      "|            304.75|     25.0|\n",
      "|115.56119441986084|     9.48|\n",
      "| 950.8199999999999|     78.0|\n",
      "|189.55450232505797|    15.55|\n",
      "|6082.8099999999995|    499.0|\n",
      "|            1462.8|    120.0|\n",
      "|              null|     null|\n",
      "| 1031.273981399536|     84.6|\n",
      "|             365.7|     30.0|\n",
      "| 203.0853981399536|    16.66|\n",
      "|2072.2999999999997|    170.0|\n",
      "+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature.select(\"price\",\"org_price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+-----+\n",
      "|category_1                                                    |count|\n",
      "+--------------------------------------------------------------+-----+\n",
      "|Mobiles & Tablets                                             |7273 |\n",
      "|Home & Living                                                 |6042 |\n",
      "|Fashion                                                       |5729 |\n",
      "|Watches Sunglasses Jewellery                                  |4216 |\n",
      "|Health & Beauty                                               |4040 |\n",
      "|Computers & Laptops                                           |2882 |\n",
      "|TV, Audio / Video, Gaming & Wearables                         |2505 |\n",
      "|Cameras                                                       |1950 |\n",
      "|Home Appliances                                               |1583 |\n",
      "| 8GB                                                          |5    |\n",
      "| 4GB                                                          |5    |\n",
      "|A1466)                                                        |2    |\n",
      "| Black\"                                                       |2    |\n",
      "| Direct Shipping &amp; Free Installation in Klang Valley\"     |2    |\n",
      "|Soft-Touch Plastic 13 inch Cover for Apple MacBook Pro 13.3\"\"\"|1    |\n",
      "|W7Pro)\"                                                       |1    |\n",
      "|W10\"                                                          |1    |\n",
      "| Crystal-Black)\"                                              |1    |\n",
      "| BLACK + ASUS ORIGINAL DESIGN BACKPACK\"                       |1    |\n",
      "| W8.1) - FREE Office 365 Personal + Targus Backpack\"          |1    |\n",
      "+--------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "des_cat_test.groupBy(\"category_1\").count().orderBy(\"count\",ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-32-8532e3f6520c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-8532e3f6520c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cat_train_test = des_cat_test.join(main_category,des_cat_test.category_1=main_category.category_1, 'inner')\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "main_category=des_cat_test.groupBy(\"category_1\").count().filter(\"count>5\")\n",
    "cat_train_test = des_cat_test.join(main_category,des_cat_test.category_1=main_category.category_1, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            sku_id|\n",
      "+-------+------------------+\n",
      "|  count|             36283|\n",
      "|   mean|              null|\n",
      "| stddev|              null|\n",
      "|    min|1A127ELAA3U0JKANPH|\n",
      "|    max|  ZY795HLCZ3O0ANMY|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature.select(\"sku_id\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=spark.read.csv(\"../Clean_Data_Frame.csv\",escape='\"',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=train_data.filter(train_data[\"category_1\"]==\"Fashion\").drop(\"category_1\",\"org_price\",\"clarity\",\"country\")\n",
    "test_sample=test_sample.na.fill({\"description\":\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer,StopWordsRemover,CountVectorizer\n",
    "from pyspark.sql.types import StructField,StringType,FloatType,StructType,IntegerType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "def useful_token_func(col1,col2):\n",
    "    if len(col1)>col2:\n",
    "        return 0\n",
    "    else:\n",
    "        return col2-len(col1)\n",
    "useful_token = udf(useful_token_func, IntegerType())\n",
    "\n",
    "test_1 = test_sample.withColumn(\"title_no_sc1\", regexp_replace(test_sample[\"title\"], '[-/()]+', ' '))\n",
    "test_1 = test_1.withColumn(\"title_no_sc\", regexp_replace(test_1[\"title_no_sc1\"], '''['.]+''', ''))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title_no_sc\", outputCol=\"words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title_no_sc\", outputCol=\"regex_words\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"regex_words\", outputCol=\"no_stopwords\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "test_1 = tokenizer.transform(test_1)\n",
    "test_1=test_1.withColumn(\"tokens\", countTokens(test_1[\"words\"]))\n",
    "\n",
    "test_1 = regexTokenizer.transform(test_1)\n",
    "test_1=test_1.withColumn(\"regex_tokens\", useful_token(test_1[\"regex_words\"],test_1[\"tokens\"]))\n",
    "\n",
    "test_1 = stop_words_remover.transform(test_1)\n",
    "test_1=test_1.withColumn(\"sw_tokens\", useful_token(test_1[\"no_stopwords\"],test_1[\"tokens\"]))\n",
    "test_1=test_1.withColumn(\"nsw_tokens\", countTokens(test_1[\"no_stopwords\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1=test_1.drop(\"regex/org\",\"sw/org\",\"title\",\"title_no_sc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test_1.withColumn(\"desc_no_sc1\", regexp_replace(test_sample[\"description\"], '[-/()]+', ' '))\n",
    "test_2 = test_2.withColumn(\"desc_no_sc\", regexp_replace(test_2[\"desc_no_sc1\"], '''['.]+''', ''))\n",
    "\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"desc_no_sc\", outputCol=\"regex_des\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"regex_des\", outputCol=\"no_stopwords_des\")\n",
    "\n",
    "test_2 = regexTokenizer.transform(test_2)\n",
    "test_2 = stop_words_remover.transform(test_2)\n",
    "test_2=test_2.withColumn(\"des_tokens\", countTokens(test_2[\"no_stopwords_des\"]))\n",
    "test_2=test_2.withColumn(\"r_tit_des\",test_2[\"nsw_tokens\"]/test_2[\"des_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType,StringType\n",
    "uniqueExtract = udf(lambda words: list(set(words)),ArrayType(StringType()))\n",
    "test_3=test_2.withColumn(\"unique_words\",uniqueExtract(test_2[\"no_stopwords\"]))\n",
    "test_3=test_3.withColumn(\"count_repeat\",countTokens(test_3[\"no_stopwords\"])-test_3[\"nsw_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|  [shirt]| 285|\n",
      "|  [women]| 379|\n",
      "|  [dress]| 298|\n",
      "|   [blue]| 312|\n",
      "| [casual]| 287|\n",
      "|   [intl]| 475|\n",
      "|   [long]| 257|\n",
      "| [sleeve]| 275|\n",
      "|[fashion]| 319|\n",
      "|   [mens]| 287|\n",
      "|  [black]| 542|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpGrowth = FPGrowth(itemsCol=\"unique_words\", minSupport=0.1, minConfidence=0.5)\n",
    "test_true=test_3.filter(\"concise==1\")\n",
    "fp_model = fpGrowth.fit(test_true)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "fp_model.freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|           items|freq|\n",
      "+----------------+----+\n",
      "|       [leather]| 443|\n",
      "|        [sleeve]| 391|\n",
      "|          [intl]| 784|\n",
      "|   [intl, women]| 348|\n",
      "|          [long]| 458|\n",
      "|         [dress]| 374|\n",
      "|         [black]| 627|\n",
      "|          [mens]| 384|\n",
      "|           [new]| 452|\n",
      "|          [blue]| 326|\n",
      "|       [fashion]| 805|\n",
      "|[fashion, women]| 376|\n",
      "|           [men]| 422|\n",
      "|          [sexy]| 433|\n",
      "|        [womens]| 388|\n",
      "|        [casual]| 634|\n",
      "|           [bag]| 458|\n",
      "|         [shoes]| 335|\n",
      "|         [women]|1215|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"nsw_words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(test_3)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = test_sample.withColumn(\"title_no_sc\", regexp_replace(test_sample[\"title\"], '[-]+', ' '))\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "test_1 = tokenizer.transform(test_sample)\n",
    "test_1=test_1.withColumn(\"tokens\", countTokens(test_1[\"words\"]))\n",
    "\n",
    "test_1 = regexTokenizer.transform(test_1)\n",
    "test_1=test_1.withColumn(\"regex_tokens\", countTokens(test_1[\"regex_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|               title|          category_1|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|               11417|               11417|\n",
      "|   mean|              2016.0|                null|\n",
      "| stddev|                 NaN|                null|\n",
      "|    min|!!! -1.00 !!! Lar...|             Cameras|\n",
      "|    max|���Always Smile�۝...|Watches Sunglasse...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|               title|          category_1|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|               34228|               34228|\n",
      "|   mean|                null|                null|\n",
      "| stddev|                null|                null|\n",
      "|    min|!!! -1.00 !!! Lar...|             Cameras|\n",
      "|    max|���Always Smile�۝...|Watches Sunglasse...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.filter(\"clarity==1\").select(\"title\",\"category_1\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|               title|          category_1|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|                2055|                2055|\n",
      "|   mean|              2016.0|                null|\n",
      "| stddev|                 NaN|                null|\n",
      "|    min|\"ASTAR Lady's Str...|             Cameras|\n",
      "|    max|yukufus MEGIR min...|Watches Sunglasse...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.filter(\"clarity==0\").select(\"title\",\"category_1\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
